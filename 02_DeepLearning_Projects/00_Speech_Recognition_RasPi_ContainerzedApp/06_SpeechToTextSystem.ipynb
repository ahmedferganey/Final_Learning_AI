{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "[Audio Data](https://drive.google.com/drive/folders/1802iY9TDPscgavH7Alz_MxjznL8tmV8O) | [whisper GitHub](https://github.com/openai/whisper/blob/main/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ferganey/spechmdl/lib/python3.12/site-packages (24.3.1)\n",
      "Requirement already satisfied: setuptools in /home/ferganey/spechmdl/lib/python3.12/site-packages (75.6.0)\n",
      "Requirement already satisfied: wheel in /home/ferganey/spechmdl/lib/python3.12/site-packages (0.45.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /home/ferganey/spechmdl/lib/python3.12/site-packages (20240930)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper --no-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ferganey/spechmdl/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: tqdm in /home/ferganey/spechmdl/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: tiktoken in /home/ferganey/spechmdl/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: more-itertools in /home/ferganey/spechmdl/lib/python3.12/site-packages (10.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tqdm tiktoken more-itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /home/ferganey/spechmdl/lib/python3.12/site-packages (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from numba) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu/\n",
      "Requirement already satisfied: torch in /home/ferganey/spechmdl/lib/python3.12/site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: filelock in /home/ferganey/spechmdl/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ferganey/spechmdl/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ferganey/spechmdl/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ferganey/spechmdl/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ferganey/spechmdl/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install triton>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in /home/ferganey/spechmdl/lib/python3.12/site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240930\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "print(whisper.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferganey/spechmdl/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "# Load the tiny model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 512)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the model is on the CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferganey/spechmdl/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shopping at the mall. At one time in North America most people shopped downtown on Main Street. Most businesses were at the center of town. When people started using automobiles however they moved away from downtown. In time most people lived in the suburbs. Eventually stores and small shopping plazas were built in suburban areas. Still most of the big stores were downtown. But as more and more cars were on the roads driving and parking downtown became a problem. There wasn't room for a lot of cars to park downtown. People also didn't want to fight downtown traffic just to go shopping. So in the 1950s and 1960s there was the beginning of large suburban shopping malls and plazas. Plazas were a row of stores attached to one another. Malls were usually a double row of stores with a roof connecting both rows. This means that shoppers did all their shopping inside. Large department stores and grocery stores were usually part of the mall but there were many smaller stores as well. When you came to the mall and went inside many people would get a shopping cart. You can walk along the aisles putting your purchases in the cart. When you're finished shopping you can push your shopping buggy out to the car. Many malls also have buggies or strollers for pushing small children along. There can be a lot of walking in a trip to the mall. In fact some people go to the mall just to exercise. A half dozen laps surround the mall every morning amount to a pretty good work out. However there are always places to sit down when you get tired. Most malls have a food court. This is an open area with a lot of tables and chairs. Usually there are a dozen or more small restaurants circled around the food court. The department store is often half full-size restaurants. Malls have large parking lots. Unlike downtown you don't pay to park at the mall. On a busy day, finding a space close to the store can be a challenge. Many people go to the malls when the weather is bad. During which we weather the malls are busy. Likewise, in really hot summer weather people go to the malls to get cool. The climate there is always the same. People don't go to malls just to shop. They also go to meet people. Usually you bump into friends and neighbors there. Old people as well as teenagers go there to see friends. Usually the malls sponsor special events. With lots to see and do malls are a popular place to hang out.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transcribe an audio file\n",
    "result = model.transcribe(\"/media/ferganey/Data1/03_Projects/Final_Learning_AI-Projects/02_DeepLearning_Projects/00_Speech_Recognition_RasPi_ContainerzedApp/01_Dataset/00_Audio/shopping.mp3\")\n",
    "\n",
    "# Print the transcribed text\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"/media/ferganey/Data1/03_Projects/Final_Learning_AI-Projects/02_DeepLearning_Projects/00_Speech_Recognition_RasPi_ContainerzedApp/01_Dataset/00_Audio/shopping.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "shopping at the mall. At one time in North America most people shopped downtown on Main Street. Most businesses were at the center of town. When people started using automobiles however they moved away from downtown. In time most people lived in the suburbs. Eventually stores and small shopping plazas were built in suburban areas. Still most of the big stores were downtown. But as more and more cars were on the roads driving and parking downtown became a problem. There\n"
     ]
    }
   ],
   "source": [
    "# Only take the first 30 seconds with the correct sampling rate (16000 Hz)\n",
    "max_duration = 30  # 30 seconds\n",
    "sample_rate = 16000  # Whisper's default sampling rate\n",
    "audio = audio[:max_duration * sample_rate]  # Slice the first 30 seconds\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# Generate log-Mel spectrogram\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# Detect the language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# Decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# Output the result\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 71825920\n",
      "Total Model Size: 273.99 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the size in bytes\n",
    "total_params = sum(p.numel() for p in model.parameters())  # Total number of parameters\n",
    "total_size_bytes = total_params * 4  # Assuming 4 bytes per parameter (float32)\n",
    "\n",
    "# Convert bytes to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Total Model Size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-5): 6 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TextDecoder(\n",
      "    (token_embedding): Embedding(51865, 512)\n",
      "    (blocks): ModuleList(\n",
      "      (0-5): 6 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Verify all model weights are float32\n",
    "all([param.dtype == torch.float32 for name, param in model.named_parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spechmdl)",
   "language": "python",
   "name": "spechmdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
