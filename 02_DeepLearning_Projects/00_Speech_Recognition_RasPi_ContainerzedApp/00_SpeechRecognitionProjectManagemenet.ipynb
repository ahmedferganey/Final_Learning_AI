{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Objective**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Build and containerize a Docker image with:\n",
    "\n",
    "- **Speech recognition** (using Vosk).  \n",
    "- **Intent detection logic** to analyze the audio commands.  \n",
    "- **Real-time audio input** from a microphone.  \n",
    "- **Output context** (printed logs or CAN bus communication).  \n",
    "\n",
    "---\n",
    "\n",
    "## Steps to Set Up the Containerized AI Component\n",
    "\n",
    "### Step 1: Prepare the Application\n",
    "\n",
    "You'll need a speech recognition system that:\n",
    "\n",
    "1. **Captures audio via a microphone.**  \n",
    "2. **Processes speech using Vosk** to recognize Arabic (or other commands).  \n",
    "3. **Determines context/intent** from the recognized audio.  \n",
    "4. **Sends this context as the application output.**  \n",
    "\n",
    "##### Write `main.py`  \n",
    "\n",
    "Below is the full application logic focused only on:\n",
    "\n",
    "- **Speech recognition using Vosk.**  \n",
    "- **NLP processing for intent recognition.**  \n",
    "- **Output context** (via logs or other communication mechanisms).  \n",
    "\n",
    "```Python\n",
    "import sounddevice as sd\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import can\n",
    "\n",
    "\n",
    "# Load Vosk model\n",
    "model = Model(\"vosk-model-arabic\")\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "# Function to process intent logic\n",
    "commands = {\n",
    "    \"turn on the air conditioning\": \"control_hvac\",\n",
    "    \"navigate to work\": \"start_navigation\",\n",
    "    \"what's the weather\": \"check_weather\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_intent(text):\n",
    "    \"\"\"Analyze recognized text and map to specific intent.\"\"\"\n",
    "    text = text.lower()\n",
    "    for command, intent in commands.items():\n",
    "        if command in text:\n",
    "            return intent\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def send_context_to_can(intent):\n",
    "    \"\"\"\n",
    "    Simulate sending the processed intent to CAN bus\n",
    "    Replace with actual CAN communication if available.\n",
    "    \"\"\"\n",
    "    print(f\"[CAN Message Sent]: {intent}\")\n",
    "\n",
    "\n",
    "def process_audio_result(text):\n",
    "    \"\"\"\n",
    "    Take recognized audio and process intent logic.\n",
    "    \"\"\"\n",
    "    intent = get_intent(text)\n",
    "    send_context_to_can(intent)\n",
    "\n",
    "\n",
    "# Callback to handle audio data\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if recognizer.AcceptWaveform(indata):\n",
    "        result = json.loads(recognizer.Result())\n",
    "        text = result.get(\"text\", \"\")\n",
    "        if text:\n",
    "            print(f\"[Speech Input]: {text}\")\n",
    "            process_audio_result(text)\n",
    "\n",
    "\n",
    "# Real-time audio capture\n",
    "with sd.RawInputStream(\n",
    "    samplerate=16000,\n",
    "    blocksize=8000,\n",
    "    dtype=\"int16\",\n",
    "    channels=1,\n",
    "    callback=audio_callback,\n",
    "):\n",
    "    print(\"Listening... Speak now.\")\n",
    "    sd.sleep(-1)  # Keep listening indefinitely\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "##### Write `the requirements.txt`\n",
    "List the Python dependencies.\n",
    "\n",
    "```plaintext\n",
    "vosk\n",
    "sounddevice\n",
    "python-can\n",
    "json\n",
    "\n",
    "```\n",
    "\n",
    "### Step 2: Create the Dockerfile  \n",
    "The `Dockerfile` builds the application container and sets up necessary dependencies.\n",
    "\n",
    "**Dockerfile**\n",
    "```dockerfile\n",
    "# Use the Raspberry Pi compatible base image with Python\n",
    "FROM arm64v8/python:3.9\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy application dependencies\n",
    "COPY requirements.txt /app/\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the AI application logic\n",
    "COPY . /app/\n",
    "\n",
    "# Default command to start the application\n",
    "CMD [\"python\", \"main.py\"]\n",
    "```\n",
    "\n",
    "### Step 3: Build the Docker Image  \n",
    "Now that you have `Dockerfile`, `main.py`, and `requirements.txt`, build the Docker image.\n",
    "\n",
    "```bash\n",
    "docker build -t ai_audio_processor .\n",
    "```\n",
    "\n",
    "### Step 4: Run the Container  \n",
    "Start the container. You'll likely need access to the host's audio devices and permissions for sound.\n",
    "```bash\n",
    "docker run --rm -it --device /dev/snd:/dev/snd ai_audio_processor\n",
    "```\n",
    "---\n",
    "\n",
    "#### Optional: Test with Audio Devices  \n",
    "Debugging permissions for the microphone:\n",
    "\n",
    "Ensure the container has access to the **Raspberry Pi microphone device** (`/dev/snd`).  \n",
    "The `--device` flag allows access to that device.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of the Workflow\n",
    "\n",
    "### Container AI App:  \n",
    "- Listens to **real-time audio input** via the microphone.  \n",
    "- Uses **Vosk speech recognition** to transcribe audio.  \n",
    "- Parses text for commands like:  \n",
    "  - \"turn on the air conditioning\"  \n",
    "  - \"navigate to work\"  \n",
    "- Sends results to **logs** or **CAN bus** using mock CAN communication.\n",
    "\n",
    "---\n",
    "\n",
    "### Docker Container Deployment:  \n",
    "- Built from a **Raspberry Pi-compatible base image.**  \n",
    "- Runs the AI logic in a **real-time streaming audio loop.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üöÄ AI Voice-Controlled Automotive Assistant Using RL**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìå **Objective**\n",
    "\n",
    "Create an AI-powered voice assistant for automotive use that listens to driver commands, processes audio input, identifies intents using reinforcement learning, and communicates with a CAN bus interface for vehicle command execution.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ **Key Features**\n",
    "\n",
    "- **Real-Time Speech Recognition**: Convert driver speech into text using **Vosk**.\n",
    "- **Intent Analysis via Reinforcement Learning (RL)**: Train an RL agent to identify user intent.\n",
    "- **CAN Bus Communication**: Send commands (e.g., turn on A/C, navigate, perform vehicle actions).\n",
    "- **Dockerized Deployment**: Run the AI model and application in a lightweight Docker container on Raspberry Pi-like devices.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **Step-by-Step Implementation Plan**\n",
    "\n",
    "### **Step 1: Set Up Development Environment**\n",
    "\n",
    "1. **Install Required Dependencies**\n",
    "   Ensure that all necessary packages are available. These will include Vosk for speech recognition, stable-baselines3 for RL implementation, Python-can for CAN bus communication, and sounddevice for real-time audio input.\n",
    "\n",
    "   ```bash\n",
    "   pip install vosk stable-baselines3 gym python-can sounddevice torch\n",
    "    ```\n",
    "\n",
    "2. **Set Up Speech-to-Text Pipeline with Vosk**:  \n",
    "   Implement a pipeline to capture real-time audio from a connected microphone, process it using the Vosk model, and convert it into text. Ensure Vosk is configured properly and works seamlessly in real-time.\n",
    "\n",
    "**Test basic Vosk transcription:**\n",
    "\n",
    "```python\n",
    "import vosk\n",
    "import sys\n",
    "import sounddevice as sd\n",
    "import queue\n",
    "import json\n",
    "\n",
    "# Load model\n",
    "model = vosk.Model(\"model\")\n",
    "\n",
    "```\n",
    "\n",
    "**Create Environment Simulation:***\n",
    "\n",
    "Using OpenAI Gym to simulate driving actions and responses.\n",
    "Define custom environments to represent commands (e.g., \"Turn on A/C\", \"Navigate to Work\").\n",
    "```python\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class DrivingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(5)  # Example: 5 different commands\n",
    "        self.observation_space = spaces.Box(-1, 1, (1,))\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 1 if action == 1 else -1\n",
    "        next_state = np.random.rand(1)  # Simulate observation\n",
    "        done = False\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.random.rand(1)\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è **Step 2: Train Your RL Model**\n",
    "\n",
    "1. **Set Up Your Gym Environment**:  \n",
    "   Simulate driving commands and driving behaviors in a controlled environment. Define an RL environment using the OpenAI Gym framework, specifying observation and action spaces that mimic real-world driving decisions.\n",
    "\n",
    "2. **Train the Model Using Stable-Baselines3**:  \n",
    "   Train an RL agent (e.g., using PPO or another RL algorithm) on the custom driving environment. The RL agent will learn to map audio-based commands to actions through trial and error, optimizing its strategy for interpreting driver intents.\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = DrivingEnv()  # Load your environment\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100000)  # Train model\n",
    "model.save(\"trained_model\")\n",
    "```\n",
    "\n",
    "3. **Save the Trained Model**:  \n",
    "   After successful training, save the trained RL model. This model will later be loaded in the containerized application for real-time inference.\n",
    "\n",
    "---\n",
    "\n",
    "### üê≥ **Step 3: Containerize the RL Model with Docker**\n",
    "\n",
    "1. **Create a Dockerfile**:  \n",
    "   Define the application environment using a Dockerfile. This Docker image will include all necessary dependencies for Vosk, stable-baselines3, sounddevice, and RL models.\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install vosk stable-baselines3 sounddevice python-can torch gym\n",
    "\n",
    "# Copy RL model\n",
    "COPY ./trained_model /app/model/\n",
    "\n",
    "WORKDIR /app\n",
    "COPY . .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "2. **Build the Docker Image**:  \n",
    "   Use the Docker CLI to build an image that encapsulates all required dependencies, the trained RL model, and application logic.\n",
    "```bash\n",
    "docker build -t rl_voice_assistant .\n",
    "```\n",
    "3. **Run the Container**:  \n",
    "   Deploy the container using the built image to ensure it runs locally or on a Raspberry Pi-like device. The container will handle audio input processing and RL inference.\n",
    "****\n",
    "```python\n",
    "import vosk\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO.load(\"model\")\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    # Real-time audio processing logic\n",
    "    print(\"Audio input received\")\n",
    "    # Process audio here with Vosk and predict action\n",
    "```\n",
    "\n",
    "**Write app.py for the Docker Entry Point:**\n",
    "\n",
    "    - Integrate Vosk, real-time audio input, and decision-making logic.\n",
    "\n",
    "```python\n",
    "import vosk\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO.load(\"model\")\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    # Real-time audio processing logic\n",
    "    print(\"Audio input received\")\n",
    "    # Process audio here with Vosk and predict action\n",
    "```\n",
    "\n",
    "**Run the Container:**\n",
    "\n",
    "```bash\n",
    "docker run -it --rm rl_voice_assistant\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è **Step 4: Integrate CAN Bus Communication**\n",
    "\n",
    "1. **Set Up CAN Bus Communication**:  \n",
    "   Implement communication protocols to send messages to the vehicle's CAN bus. Use the `python-can` library to interface with CAN communication standards.\n",
    "```python\n",
    "import can\n",
    "\n",
    "bus = can.interface.Bus(channel='can0', bustype='socketcan')\n",
    "\n",
    "def send_can_message(action):\n",
    "    msg = can.Message(arbitration_id=0x123, data=[action], is_extended_id=False)\n",
    "    bus.send(msg)\n",
    "```\n",
    "2. **Map RL Output to CAN Bus Commands**:  \n",
    "   Translate the RL model's predicted actions into actionable CAN bus commands. These commands will interface with vehicle features (e.g., activating A/C, navigation, or other driving assistance systems).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Next Steps**\n",
    "\n",
    "### 1. **Test & Debug Deployment**:\n",
    "   - Monitor system performance on the Raspberry Pi-like device.\n",
    "   - Ensure CAN bus communication reliably executes commands.\n",
    "   \n",
    "### 2. **Optimize RL Model**:\n",
    "   - Use real-time feedback from user interactions to retrain and fine-tune the RL model.\n",
    "\n",
    "### 3. **Implement Feedback Loops**:\n",
    "   - Establish continuous learning mechanisms to improve the AI's understanding of driving commands over time.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Dependencies**\n",
    "\n",
    "- **Vosk**: For real-time speech recognition and audio-to-text conversion.\n",
    "- **Stable-Baselines3**: RL library for training decision-making models.\n",
    "- **Python-Can**: For CAN bus communication with vehicle systems.\n",
    "- **Docker**: To ensure deployment is consistent and modular.\n",
    "- **Sounddevice**: Handles real-time audio capture from a connected microphone.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Deployment Plan**\n",
    "\n",
    "1. Train and validate the RL agent locally.\n",
    "2. Create and build a Docker image with all dependencies, models, and application logic.\n",
    "3. Deploy the Docker container on the Raspberry Pi-like system.\n",
    "4. Capture real-time audio input from the microphone.\n",
    "5. Pass audio input into the RL model for intent recognition.\n",
    "6. Translate the RL output into commands and send these messages via CAN bus to perform specific actions.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Summary**\n",
    "\n",
    "Following these steps will result in a fully operational voice assistant capable of listening to driver commands, processing them using reinforcement learning, and executing actions via CAN bus communication. The entire system will be deployed inside a Docker container running on a Raspberry Pi-like device to ensure consistency and portability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä **Work Breakdown Structure (WBS) for AI Voice-Controlled Automotive Assistant**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üéØ **1. Project Initiation**\n",
    "\n",
    "- **1.1 Define Objectives**  \n",
    "  - Set up goals for speech recognition, RL integration, and CAN bus communication.\n",
    "  - Establish the scope of the Dockerized RL voice assistant.\n",
    "\n",
    "- **1.2 Define Key Features**  \n",
    "  - Real-time speech recognition with Vosk.\n",
    "  - Intent analysis with reinforcement learning (RL).\n",
    "  - CAN bus communication for vehicle command execution.\n",
    "\n",
    "- **1.3 Assemble the Team**  \n",
    "  - Assign roles and responsibilities for development, testing, and deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è **2. Set Up Speech-to-Text Pipeline**\n",
    "\n",
    "### **2.1 Select Speech Recognition Model**  \n",
    "  - Research and configure Vosk for real-time transcription.\n",
    "  \n",
    "### **2.2 Capture Real-Time Audio**  \n",
    "  - Set up sound device integration for live audio capture.\n",
    "\n",
    "### **2.3 Test and Validate Audio Input with Vosk**  \n",
    "  - Run initial speech-to-text pipeline tests.\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è **3. Train RL Model**\n",
    "\n",
    "### **3.1 Create a Simulation Environment (Gym)**  \n",
    "  - Define observation space and action space for driving simulation.\n",
    "\n",
    "### **3.2 Train the RL Agent**  \n",
    "  - Train the agent with PPO or other RL methods using Stable-Baselines3.\n",
    "\n",
    "### **3.3 Validate the Model**  \n",
    "  - Run testing scenarios to verify decision-making accuracy.\n",
    "\n",
    "### **3.4 Save Trained Model**  \n",
    "  - Store the model for integration into the application pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## üê≥ **4. Containerize RL Model with Docker**\n",
    "\n",
    "### **4.1 Write Dockerfile**  \n",
    "  - Specify environment dependencies (Vosk, stable-baselines3, python-can, sounddevice).\n",
    "\n",
    "### **4.2 Build the Docker Image**  \n",
    "  - Compile all dependencies, code, and the trained RL model.\n",
    "\n",
    "### **4.3 Test Docker Container**  \n",
    "  - Ensure the container runs correctly on local machines or Raspberry Pi-like hardware.\n",
    "\n",
    "### **4.4 Debug & Optimize**  \n",
    "  - Debug any issues related to container dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **5. Set Up CAN Bus Communication**\n",
    "\n",
    "### **5.1 Research CAN Bus Architecture**  \n",
    "  - Understand CAN bus standards and communication types.\n",
    "\n",
    "### **5.2 Integrate `python-can` Library**  \n",
    "  - Write code to send CAN bus messages.\n",
    "\n",
    "### **5.3 Map RL Agent Actions to CAN Commands**  \n",
    "  - Create logic to translate RL agent intent into actionable CAN bus messages.\n",
    "\n",
    "### **5.4 Test CAN Bus Communication**  \n",
    "  - Simulate CAN bus messages to ensure commands execute correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ **6. Testing & Validation**\n",
    "\n",
    "### **6.1 Speech Recognition Validation**  \n",
    "  - Test audio capture pipeline with Vosk transcription.\n",
    "\n",
    "### **6.2 RL Model Validation**  \n",
    "  - Validate the model's intent recognition and response.\n",
    "\n",
    "### **6.3 CAN Bus Communication Validation**  \n",
    "  - Simulate sending CAN messages and validate with a test vehicle or CAN bus simulator.\n",
    "\n",
    "### **6.4 Integration Testing**  \n",
    "  - Test the entire pipeline together: audio ‚Üí intent processing ‚Üí CAN bus communication.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **7. Deployment**\n",
    "\n",
    "### **7.1 Prepare the Raspberry Pi-Like Hardware**  \n",
    "  - Ensure all drivers and dependencies are configured for deployment.\n",
    "\n",
    "### **7.2 Deploy Docker Container**  \n",
    "  - Load and deploy the pre-built container onto the target device.\n",
    "\n",
    "### **7.3 Monitor Deployment**  \n",
    "  - Observe and debug during deployment on real hardware.\n",
    "\n",
    "### **7.4 Optimize Performance**  \n",
    "  - Use feedback loops and real-time monitoring to optimize performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Dependencies**\n",
    "\n",
    "- **Vosk**: For real-time speech recognition.  \n",
    "- **Stable-Baselines3**: To implement and train the RL model.  \n",
    "- **Python-Can**: For sending and receiving CAN bus messages.  \n",
    "- **Docker**: To containerize and deploy the entire application.  \n",
    "- **Sounddevice**: Captures live audio input.  \n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **Next Steps**\n",
    "\n",
    "- **Research and prepare datasets for training**.\n",
    "- **Set up the development pipeline** for speech-to-text transcription and CAN bus logic testing.\n",
    "- **Train RL agent in simulation** using Gym.\n",
    "- **Containerize the entire AI system** using Docker and test deployment on the target hardware.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Summary**\n",
    "\n",
    "By dividing the project into these key phases and tasks using a WBS, we create clear objectives and manageable deliverables. This structured approach will lead to a successful deployment of an AI-powered voice-controlled assistant for automotive use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä **Milestones & Deadlines for AI Voice-Controlled Automotive Assistant Project**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| **Milestone**                          | **Tasks**                                                                 | **Deadline**       | **Responsible Team Member(s)** |\n",
    "|-----------------------------------------|------------------------------------------------------------------------|--------------------|----------------------------------|\n",
    "| **1. Project Initiation**               | - Define objectives and goals                                           | Week 1             | Project Manager                |\n",
    "|                                         | - Identify key features                                                 |                    |                                  |\n",
    "|                                         | - Assemble development team                                             |                    |                                  |\n",
    "| **2. Set Up Speech-to-Text Pipeline**   | - Select Vosk as the speech recognition model                          | Week 2             | Lead AI Developer              |\n",
    "|                                         | - Capture real-time audio using sounddevice                            |                    |                                  |\n",
    "|                                         | - Validate and test Vosk transcription pipeline                        |                    |                                  |\n",
    "| **3. Train RL Model**                   | - Create simulation environment with Gym                               | Week 4             | RL Developer                   |\n",
    "|                                         | - Train RL agent with Stable-Baselines3                                |                    |                                  |\n",
    "|                                         | - Validate and optimize the trained RL model                          |                    |                                  |\n",
    "| **4. Containerize RL Model with Docker** | - Write Dockerfile                                                     | Week 5             | DevOps Engineer               |\n",
    "|                                         | - Build and test Docker image                                           |                    |                                  |\n",
    "|                                         | - Debug any container dependencies                                    |                    |                                  |\n",
    "| **5. Set Up CAN Bus Communication**    | - Understand and set up the CAN bus system                             | Week 6             | Embedded Systems Engineer      |\n",
    "|                                         | - Integrate `python-can` for communication                             |                    |                                  |\n",
    "|                                         | - Map RL agent's actions to CAN bus commands                           |                    |                                  |\n",
    "| **6. Testing & Validation**             | - Test speech recognition pipeline                                      | Week 7             | QA Engineer                    |\n",
    "|                                         | - Test RL model performance                                            |                    |                                  |\n",
    "|                                         | - Test CAN bus communication                                           |                    |                                  |\n",
    "|                                         | - Integration testing of all components                               |                    |                                  |\n",
    "| **7. Deployment**                       | - Prepare hardware (Raspberry Pi-like system)                         | Week 8             | Hardware Engineer              |\n",
    "|                                         | - Deploy Docker container                                               |                    |                                  |\n",
    "|                                         | - Monitor system performance and debug                              |                    |                                  |\n",
    "|                                         | - Optimize RL model and performance feedback loops                    |                    |                                  |\n",
    "| **Final Review & Handover**            | - Perform a final system review                                        | Week 9             | Project Manager                |\n",
    "|                                         | - Handover to production and stakeholders                              |                    |                                  |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **Notes**\n",
    "- **Dependencies:** Ensure all hardware and software dependencies are met before deadlines.\n",
    "- **Adjustments:** Timelines may be adjusted based on testing feedback.\n",
    "- **Regular Check-Ins:** Weekly team meetings to assess progress and address risks.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Outcome**\n",
    "This table provides a clear view of project milestones, their associated tasks, and deadlines, making it easier to track progress and ensure timely delivery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è **Software Life Cycle Model Used**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîÑ **Iterative and Incremental Model**\n",
    "\n",
    "The **Iterative and Incremental Model** is the software development life cycle approach applied to this project. This model is ideal because it allows for the development of smaller, functional modules iteratively while incorporating testing, feedback, and refinement in each phase.\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20240318095921/Phases-of-Iterative-Incremental-Model.webp)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **Why Iterative & Incremental Model for This Project?**\n",
    "The project's requirements involve:\n",
    "- Real-time speech recognition.\n",
    "- Reinforcement learning agent training and testing.\n",
    "- CAN bus communication integration.\n",
    "- Deployment on hardware with real-time input/output feedback loops.\n",
    "\n",
    "These aspects are modular, allowing each to be developed, tested, and improved incrementally.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **Key Phases in the Iterative & Incremental Model**\n",
    "\n",
    "1. **Requirement Gathering & Analysis**\n",
    "   - Identify objectives and features (speech recognition, RL model, CAN bus integration).\n",
    "   - Gather hardware and software dependencies.\n",
    "\n",
    "2. **Design & Initial Setup**\n",
    "   - Design RL agent simulation and training environment.\n",
    "   - Set up initial Docker infrastructure.\n",
    "   - Plan CAN bus communication and voice recognition pipeline design.\n",
    "\n",
    "3. **Iterative Development**\n",
    "   - **Speech-to-Text Pipeline with Vosk:** Develop, test, and validate real-time speech recognition.\n",
    "   - **RL Model Training:** Train and optimize the RL agent iteratively with testing in between.\n",
    "   - **CAN Bus Integration:** Incrementally integrate CAN bus communication modules and ensure their stability.\n",
    "\n",
    "4. **Testing in Loops**\n",
    "   - Perform rigorous testing for each developed module:\n",
    "     - Speech-to-text accuracy.\n",
    "     - RL model performance and training effectiveness.\n",
    "     - CAN bus message reliability.\n",
    "\n",
    "5. **Deployment Testing**\n",
    "   - Deploy the containerized system on the target hardware.\n",
    "   - Monitor and test performance.\n",
    "   - Debug and retrain RL model if necessary based on real-world results.\n",
    "\n",
    "6. **Deployment & Handover**\n",
    "   - Ensure the system runs reliably on the target Raspberry Pi-like system.\n",
    "   - Gather feedback from testing and refine the model or system behavior if required.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Why Not Linear/Waterfall Model?**\n",
    "The Waterfall Model is unsuitable for this project because:\n",
    "- Real-time feedback and testing are essential at each step.\n",
    "- Changes are expected during development due to testing insights or hardware feedback.\n",
    "- The project involves multiple complex components (RL model, CAN bus, speech recognition) that depend on iterative testing and integration.\n",
    "\n",
    "Using the **Iterative and Incremental Model** allows for flexibility, continuous testing, and modular development.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Key Benefits of This Approach**\n",
    "- Allows early detection of integration issues.\n",
    "- Supports parallel development of modules like speech recognition and RL training.\n",
    "- Facilitates real-time hardware testing by identifying bugs incrementally.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Next Steps**\n",
    "1. Finalize initial design based on the first iteration's objectives.\n",
    "2. Begin development of modules in cycles with feedback loops.\n",
    "3. Test each phase iteratively to ensure performance and stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python CAN Bus](https://python-can.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
