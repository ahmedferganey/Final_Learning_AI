{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights1:\n",
      " [[0.57655593 0.51408891 0.65499847 0.00971623 0.59802817 0.13426049]\n",
      " [0.50328864 0.89486999 0.81969989 0.35925765 0.97410386 0.53116146]\n",
      " [0.43812297 0.38843755 0.34943935 0.1443839  0.70404024 0.22287655]]\n",
      "initial weights2:\n",
      " [[0.01471724]\n",
      " [0.84623049]\n",
      " [0.32993113]\n",
      " [0.68550576]\n",
      " [0.20658774]\n",
      " [0.1158958 ]]\n",
      "Epoch 0 Loss: 0.3989000627305253\n",
      "Epoch 1000 Loss: 0.2404367162567514\n",
      "Epoch 2000 Loss: 0.2403970672970047\n",
      "Epoch 3000 Loss: 0.2403592418500689\n",
      "Epoch 4000 Loss: 0.24032302799496322\n",
      "Epoch 5000 Loss: 0.2402882411564809\n",
      "Epoch 6000 Loss: 0.24025472063934106\n",
      "Epoch 7000 Loss: 0.24022232664558768\n",
      "Epoch 8000 Loss: 0.24019093772927436\n",
      "Epoch 9000 Loss: 0.24016044860867342\n",
      "Epoch 10000 Loss: 0.24013076827155144\n",
      "Epoch 11000 Loss: 0.24010181832139352\n",
      "Epoch 12000 Loss: 0.2400735315224976\n",
      "Epoch 13000 Loss: 0.24004585051007696\n",
      "Epoch 14000 Loss: 0.24001872663829116\n",
      "Epoch 15000 Loss: 0.23999211894472908\n",
      "Epoch 16000 Loss: 0.239965993214512\n",
      "Epoch 17000 Loss: 0.23994032113100902\n",
      "Epoch 18000 Loss: 0.23991507950328778\n",
      "Epoch 19000 Loss: 0.2398902495629372\n",
      "Final weights1:\n",
      " [[5.48561078e-01 5.24886224e-01 6.41438442e-01 6.61860335e-05\n",
      "  5.91928204e-01 1.11522186e-01]\n",
      " [4.47298932e-01 9.16464629e-01 7.92579841e-01 3.39957571e-01\n",
      "  9.61903924e-01 4.85684856e-01]\n",
      " [3.54138411e-01 4.20829500e-01 3.08759271e-01 1.15433779e-01\n",
      "  6.85740342e-01 1.54661638e-01]]\n",
      "Final weights2:\n",
      " [[-0.5558877 ]\n",
      " [ 0.43927303]\n",
      " [-0.13378321]\n",
      " [-0.06258125]\n",
      " [ 0.16105959]\n",
      " [-0.59184416]]\n",
      "Final output after training:\n",
      " [[0.40617509]\n",
      " [0.40408738]\n",
      " [0.4016819 ]\n",
      " [0.39887574]\n",
      " [0.3956519 ]]\n",
      "Expected output (y):\n",
      " [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, learning_rate=0.01):\n",
    "        self.input      = np.array(x) / np.max(x)  # Normalize inputs\n",
    "        self.weights1   = np.random.rand(self.input.shape[1], 6)  # Increased hidden layer neurons\n",
    "        self.weights2   = np.random.rand(6, 1)      \n",
    "        self.y          = np.array(y).reshape(-1, 1) / np.max(y)  # Normalize output\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    " \n",
    "    def backprop(self):\n",
    "        # Application of the chain rule to find the derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, \n",
    "                            (2 * (self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,\n",
    "                            (np.dot(2 * (self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) \n",
    "                            * sigmoid_derivative(self.layer1)))\n",
    " \n",
    "        # Update the weights with the learning rate\n",
    "        self.weights1 += self.learning_rate * d_weights1\n",
    "        self.weights2 += self.learning_rate * d_weights2\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.feedforward()\n",
    "            self.backprop()\n",
    "            if epoch % 1000 == 0:\n",
    "                # Mean Squared Error Loss\n",
    "                loss = np.mean(np.square(self.y - self.output))\n",
    "                print(f\"Epoch {epoch} Loss: {loss}\")\n",
    "\n",
    "inputs = [[1, 2, 3],\n",
    "          [2, 4, 6],\n",
    "          [3, 6, 9],\n",
    "          [4, 8, 12],\n",
    "          [5, 10, 15]]\n",
    "outputs = [0, 1, 0, 1, 0]\n",
    "\n",
    "# Initialize the neural network with a learning rate\n",
    "app_1 = NeuralNetwork(inputs, outputs, learning_rate=0.01)\n",
    "\n",
    "print(\"initial weights1:\\n\", app_1.weights1)\n",
    "print(\"initial weights2:\\n\", app_1.weights2)\n",
    "\n",
    "\n",
    "# Train the network for 20,000 epochs\n",
    "app_1.train(epochs=20000)\n",
    "\n",
    "# Display the final weights and output\n",
    "print(\"Final weights1:\\n\", app_1.weights1)\n",
    "print(\"Final weights2:\\n\", app_1.weights2)\n",
    "print(\"Final output after training:\\n\", app_1.output * np.max(outputs))  # Denormalize output\n",
    "print(\"Expected output (y):\\n\", app_1.y * np.max(outputs))  # Denormalize expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.16387844, 0.44053379, 0.67081371, 0.08014979],\n",
       "        [0.26691855, 0.56196013, 0.92606066, 0.22048417],\n",
       "        [0.72562238, 0.56651683, 0.20371703, 0.67436192]]),\n",
       " array([0., 0., 0.]),\n",
       " array([ 1,  5, 10]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork2:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = np.array(x)\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        # the 4 specifies the number of neurons in the hidden layer. Here's how it breaks down:\n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = np.array(y)\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "app_1_inputs = [[1,2,3],\n",
    "                [4,5,6],\n",
    "                [7,8,9]]\n",
    "app_1_outputs= [1,5,10]\n",
    "\n",
    "app_1 = NeuralNetwork2(app_1_inputs, app_1_outputs)\n",
    "\n",
    "app_1.weights1, app_1.output, app_1.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.5\n",
      "Epoch 1000 Loss: 0.5\n",
      "Epoch 2000 Loss: 0.5\n",
      "Epoch 3000 Loss: 0.75\n",
      "Epoch 4000 Loss: 0.75\n",
      "Epoch 5000 Loss: 0.75\n",
      "Epoch 6000 Loss: 0.5\n",
      "Epoch 7000 Loss: 0.25\n",
      "Epoch 8000 Loss: 0.5\n",
      "Epoch 9000 Loss: 0.25\n",
      "Final weights1:\n",
      " [[ 4.92701620e-01  4.90674953e-01  3.43392488e-01  8.48144441e-01\n",
      "   2.96834945e-01  7.52747765e-01]\n",
      " [ 1.41610696e-01  5.74480045e-01  5.72225035e-01 -1.11810118e-04\n",
      "   2.87419395e-01  3.81680693e-01]]\n",
      "Final weights2:\n",
      " [[-0.0064345 ]\n",
      " [-0.48157486]\n",
      " [ 0.40523243]\n",
      " [ 0.11921238]\n",
      " [ 0.12236263]\n",
      " [-0.12846414]]\n",
      "Final output after training:\n",
      " [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Expected output (y):\n",
      " [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value=0, operation=None, inputs=None):\n",
    "        self.value = value  # Output of the node\n",
    "        self.grad = 0  # Gradient of the node, initialized to 0\n",
    "        self.operation = operation  # The operation that this node performs\n",
    "        self.inputs = inputs if inputs else []  # Inputs to this node (list of nodes)\n",
    "        self.output_nodes = []  # Nodes that depend on this one for their calculation\n",
    "        \n",
    "        # If the operation is set, we connect the input nodes to this one\n",
    "        if self.inputs:\n",
    "            for input_node in self.inputs:\n",
    "                input_node.output_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Compute the output of the node (forward pass). \"\"\"\n",
    "        if self.operation:\n",
    "            self.value = self.operation(*[input.value for input in self.inputs])\n",
    "        return self.value\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\" Compute the gradient of the node (backward pass). \"\"\"\n",
    "        if self.operation:\n",
    "            # Get the derivative of the operation with respect to its inputs\n",
    "            self.grad = self.operation.gradient(*[input.value for input in self.inputs])\n",
    "        \n",
    "        # Propagate the gradient to the input nodes\n",
    "        for input_node in self.inputs:\n",
    "            input_node.grad += self.grad * input_node.value  # Chain rule\n",
    "\n",
    "class Add:\n",
    "    @staticmethod\n",
    "    def __call__(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(x, y):\n",
    "        return 1, 1  # Gradient of x + y with respect to x and y\n",
    "\n",
    "class Multiply:\n",
    "    @staticmethod\n",
    "    def __call__(self, x, y):\n",
    "        return x * y\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(x, y):\n",
    "        return y, x  # Gradient of x * y with respect to x and y\n",
    "\n",
    "class ComputationalGraph:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "\n",
    "    def add_node(self, node):\n",
    "        \"\"\" Add a node to the computational graph \"\"\"\n",
    "        self.nodes.append(node)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Perform a forward pass on all nodes \"\"\"\n",
    "        for node in self.nodes:\n",
    "            node.forward()\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\" Perform a backward pass to compute gradients \"\"\"\n",
    "        # Initialize a dictionary to hold input gradients\n",
    "        input_gradients = {}\n",
    "\n",
    "        for node in reversed(self.nodes):\n",
    "            node.backward()\n",
    "\n",
    "            # Store the gradients of the input nodes in the dictionary\n",
    "            for input_node in node.inputs:\n",
    "                if input_node not in input_gradients:\n",
    "                    input_gradients[input_node] = input_node.grad\n",
    "                else:\n",
    "                    input_gradients[input_node] += input_node.grad\n",
    "        \n",
    "        return input_gradients\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, learning_rate=0.01):\n",
    "        self.input = np.array(x) / np.max(x)  # Normalize inputs\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 6)  # Increased hidden layer neurons\n",
    "        self.weights2 = np.random.rand(6, 1)      \n",
    "        self.y = np.array(y).reshape(-1, 1) / np.max(y)  # Normalize output\n",
    "        self.output = np.zeros(self.y.shape)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feedforward(self):\n",
    "        \"\"\" Forward pass to calculate the output \"\"\"\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))  # Hidden layer output\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))  # Final output\n",
    "\n",
    "        # Threshold output to get binary 0 or 1\n",
    "        self.output = np.where(self.output > 0.5, 1, 0)\n",
    "\n",
    "    def backprop(self):\n",
    "        \"\"\" Backward pass using the chain rule \"\"\"\n",
    "        # Compute error\n",
    "        output_error = self.y - self.output\n",
    "        output_delta = output_error * sigmoid_derivative(self.output)\n",
    "        \n",
    "        # Compute gradients for weights2\n",
    "        d_weights2 = np.dot(self.layer1.T, output_delta)\n",
    "        \n",
    "        # Compute error for hidden layer\n",
    "        hidden_error = output_delta.dot(self.weights2.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.layer1)\n",
    "        \n",
    "        # Compute gradients for weights1\n",
    "        d_weights1 = np.dot(self.input.T, hidden_delta)\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights1 += self.learning_rate * d_weights1\n",
    "        self.weights2 += self.learning_rate * d_weights2\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.feedforward()\n",
    "            self.backprop()\n",
    "            if epoch % 1000 == 0:\n",
    "                # Mean Squared Error Loss\n",
    "                loss = np.mean(np.square(self.y - self.output))\n",
    "                print(f\"Epoch {epoch} Loss: {loss}\")\n",
    "\n",
    "# Define input and expected output\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Create neural network\n",
    "nn = NeuralNetwork(x, y, learning_rate=0.001)\n",
    "\n",
    "# Train the neural network for 10000 epochs\n",
    "nn.train(epochs=10000)\n",
    "\n",
    "# After training, output the final weights and predictions\n",
    "print(\"Final weights1:\\n\", nn.weights1)\n",
    "print(\"Final weights2:\\n\", nn.weights2)\n",
    "print(\"Final output after training:\\n\", nn.output)\n",
    "print(\"Expected output (y):\\n\", y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
